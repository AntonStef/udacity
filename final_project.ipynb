{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from keras.utils import np_utils\n",
    "from scipy.io import loadmat "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = loadmat('./train_32x32.mat')\n",
    "test_data_set = loadmat('./test_32x32.mat')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 32\n",
    "num_labels = 10\n",
    "num_channels = 3\n",
    "\n",
    "X_train = train_data_set['X']\n",
    "y_train = train_data_set['y']\n",
    "X_test = test_data_set['X']\n",
    "y_test = test_data_set['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_train1 = X_train.reshape(X_train.shape[3], -1)\n",
    "X_train = np.transpose(X_train, (3, 0, 1, 2))\n",
    "X_test = np.transpose(X_test, (3, 0, 1, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 1\n"
     ]
    }
   ],
   "source": [
    "# check image\n",
    "plt.imshow(X_train[30000])\n",
    "plt.show()\n",
    "print(\"label {}\".format(y_train[30000].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73257, 32, 32, 3) (73257, 1)\n",
      "(26032, 32, 32, 3) (26032, 1)\n"
     ]
    }
   ],
   "source": [
    "def randomize(dataset, labels):\n",
    "  permutation = np.random.permutation(labels.shape[0])\n",
    "  shuffled_dataset = dataset[permutation,:,:]\n",
    "  shuffled_labels = labels[permutation]\n",
    "  return shuffled_dataset, shuffled_labels\n",
    "X_train, y_train = randomize(X_train, y_train)\n",
    "X_test, y_test = randomize(X_test, y_test)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 5\n"
     ]
    }
   ],
   "source": [
    "# check image\n",
    "plt.imshow(X_train[20000])\n",
    "plt.show()\n",
    "print(\"label {}\".format(y_train[20000].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "\n",
    "X_train = (X_train / 255).astype('float32')\n",
    "X_test = (X_test / 255).astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label 4\n"
     ]
    }
   ],
   "source": [
    "# check image\n",
    "plt.imshow(X_train[1])\n",
    "plt.show()\n",
    "print(\"label {}\".format(y_train[1].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(73257, 10)\n",
      "(26032, 10)\n"
     ]
    }
   ],
   "source": [
    "# one_hot coding labels\n",
    "\n",
    "y_train[np.where(y_train == 10)] = 0\n",
    "y_test[np.where(y_test == 10)] = 0\n",
    "max(y_test)\n",
    "# y_test[1:100]\n",
    "\n",
    "y_train = np_utils.to_categorical(y_train, num_labels)\n",
    "y_test = np_utils.to_categorical(y_test, num_labels)\n",
    "\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([], dtype=int64), array([], dtype=int64))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(y_train == 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8257, 32, 32, 3) (65000, 32, 32, 3) (26032, 32, 32, 3)\n",
      "(8257, 10) (65000, 10) (26032, 10)\n"
     ]
    }
   ],
   "source": [
    "# create valid_data_set\n",
    "valid_dataset = X_train[65000:].astype('float32')\n",
    "train_dataset = X_train[:65000].astype('float32')\n",
    "test_dataset = X_test.astype('float32')\n",
    "\n",
    "train_labels = y_train[:65000]\n",
    "valid_labels = y_train[65000:]\n",
    "test_labels =  y_test\n",
    "\n",
    "print(valid_dataset.shape, train_dataset.shape, test_dataset.shape)\n",
    "print(valid_labels.shape, train_labels.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8257, 32, 32, 1) (65000, 32, 32, 1) (26032, 32, 32, 1)\n"
     ]
    }
   ],
   "source": [
    "# grayscale\n",
    "def rgb2gray(dataset):\n",
    "    nex_matrix = np.zeros([dataset.shape[0], dataset.shape[1], dataset.shape[2]])\n",
    "    for i in range(dataset.shape[0]):\n",
    "        r, g, b = dataset[i,:,:,0], dataset[i,:,:,1], dataset[i, :,:,2]\n",
    "        nex_matrix[i] = 0.2989 * r + 0.5870 * g + 0.1140 * b\n",
    "    nex_matrix = nex_matrix[:, :, :, np.newaxis].astype('float32')\n",
    "    return nex_matrix\n",
    "\n",
    "train_dataset = rgb2gray(train_dataset)\n",
    "valid_dataset = rgb2gray(valid_dataset)\n",
    "test_dataset = rgb2gray(test_dataset)\n",
    "print(valid_dataset.shape, train_dataset.shape, test_dataset.shape)\n",
    "num_channels = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# check image\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "plt.imshow(train_dataset[31000].reshape(train_dataset.shape[1], train_dataset.shape[2]))\n",
    "plt.show()\n",
    "\n",
    "print(\"label {}\".format(train_labels[31000].squeeze()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8257, 1024) (65000, 1024) (26032, 1024)\n"
     ]
    }
   ],
   "source": [
    "# in line\n",
    "train_dataset = train_dataset.reshape(train_dataset.shape[0], -1)\n",
    "valid_dataset = valid_dataset.reshape(valid_dataset.shape[0], -1)\n",
    "test_dataset = test_dataset.reshape(test_dataset.shape[0], -1)\n",
    "print(valid_dataset.shape, train_dataset.shape, test_dataset.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# many_layers\n",
    "\n",
    "batch_size = 512\n",
    "hidden_units_1 = 1024\n",
    "hidden_units_2 = 512\n",
    "hidden_units_3 = 128\n",
    "# hidden_units_4 = 256\n",
    "# hidden_units_5 = 256\n",
    "# hidden_units_6 = 128\n",
    "lambd = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_units_1]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_units_1]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_units_1, hidden_units_2]))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_units_2]))\n",
    "    \n",
    "    weights3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_units_2, hidden_units_3]))\n",
    "    biases3 = tf.Variable(tf.zeros([hidden_units_3]))\n",
    "    \n",
    "    weights4 = tf.Variable(tf.truncated_normal([hidden_units_3, num_labels]))\n",
    "    biases4 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "#     weights5 = tf.Variable(\n",
    "#     tf.truncated_normal([hidden_units_4, hidden_units_5]))\n",
    "#     biases5 = tf.Variable(tf.zeros([hidden_units_5]))\n",
    "    \n",
    "#     weights6 = tf.Variable(tf.truncated_normal([hidden_units_5, hidden_units_6]))\n",
    "#     biases6 = tf.Variable(tf.zeros([hidden_units_6]))\n",
    "    \n",
    "#     weights7 = tf.Variable(tf.truncated_normal([hidden_units_6, num_labels]))\n",
    "#     biases7 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "      \n",
    "    parametrs = {'weights1': weights1,\n",
    "                 'weights2': weights2, \n",
    "                 'weights3': weights3,\n",
    "                 'weights4': weights4,\n",
    "                 'biases1': biases1,\n",
    "                 'biases2': biases2,\n",
    "                 'biases3': biases3,\n",
    "                 'biases4': biases4}\n",
    "\n",
    "    # Training computation.\n",
    "\n",
    "    def forward_propagation(tf_dataset, keep_prob, parametrs):\n",
    "      fist_level = tf.nn.relu(tf.add(tf.matmul(tf_dataset, parametrs['weights1']), parametrs['biases1']))\n",
    "      drop_out = tf.nn.dropout(fist_level, keep_prob)\n",
    "      second_level = tf.nn.relu(tf.add(tf.matmul(drop_out, parametrs['weights2']), parametrs['biases2']))\n",
    "      drop_out = tf.nn.dropout(second_level, keep_prob)\n",
    "      third_level = tf.nn.sigmoid(tf.add(tf.matmul(drop_out, parametrs['weights3']), parametrs['biases3']))\n",
    "      drop_out = tf.nn.dropout(third_level, keep_prob)\n",
    "#       fourth_level = tf.nn.sigmoid(tf.add(tf.matmul(drop_out, parametrs['weights4']), parametrs['biases4']))\n",
    "#       drop_out = tf.nn.dropout(fourth_level, keep_prob)\n",
    "#       fifth_level = tf.nn.sigmoid(tf.add(tf.matmul(drop_out, parametrs['weights5']), parametrs['biases5']))\n",
    "#       drop_out = tf.nn.dropout(fifth_level, keep_prob)\n",
    "#       six_level = tf.nn.sigmoid(tf.add(tf.matmul(drop_out, parametrs['weights6']), parametrs['biases6']))\n",
    "      out = tf.add(tf.matmul(drop_out, parametrs['weights4']), parametrs['biases4'])\n",
    "      return out\n",
    "\n",
    "    logits = forward_propagation(tf_train_dataset, keep_prob, parametrs)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Loss function using L2 Regularization\n",
    "    regularizer = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3) + tf.nn.l2_loss(weights4)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss + lambd * regularizer)\n",
    "\n",
    "    # Optimizer.\n",
    "    # Decaying learning rate\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    start_learning_rate = 0.6\n",
    "    learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 5000, 0.8)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "#     optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forward_propagation(tf_valid_dataset, 1, parametrs))\n",
    "    test_prediction = tf.nn.softmax(forward_propagation(tf_test_dataset, 1, parametrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 649.495300\n",
      "Minibatch accuracy: 10.0%\n",
      "Validation accuracy: 12.3%\n",
      "Minibatch loss at step 500: 359.745178\n",
      "Minibatch accuracy: 13.3%\n",
      "Validation accuracy: 6.2%\n",
      "Minibatch loss at step 1000: 201.241104\n",
      "Minibatch accuracy: 18.9%\n",
      "Validation accuracy: 10.5%\n",
      "Minibatch loss at step 1500: 114.600716\n",
      "Minibatch accuracy: 19.7%\n",
      "Validation accuracy: 12.1%\n",
      "Minibatch loss at step 2000: 66.494911\n",
      "Minibatch accuracy: 19.5%\n",
      "Validation accuracy: 7.5%\n",
      "Minibatch loss at step 2500: 39.541088\n",
      "Minibatch accuracy: 16.0%\n",
      "Validation accuracy: 18.6%\n",
      "Minibatch loss at step 3000: 24.639050\n",
      "Minibatch accuracy: 15.0%\n",
      "Validation accuracy: 12.1%\n",
      "Minibatch loss at step 3500: 15.511938\n",
      "Minibatch accuracy: 17.0%\n",
      "Validation accuracy: 7.5%\n",
      "Minibatch loss at step 4000: 10.258774\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 12.1%\n",
      "Minibatch loss at step 4500: 7.105705\n",
      "Minibatch accuracy: 15.4%\n",
      "Validation accuracy: 18.6%\n",
      "Minibatch loss at step 5000: 5.152337\n",
      "Minibatch accuracy: 20.5%\n",
      "Validation accuracy: 18.6%\n",
      "Minibatch loss at step 5500: 4.136101\n",
      "Minibatch accuracy: 16.4%\n",
      "Validation accuracy: 20.6%\n",
      "Minibatch loss at step 6000: 3.436621\n",
      "Minibatch accuracy: 20.5%\n",
      "Validation accuracy: 37.2%\n",
      "Minibatch loss at step 6500: 2.368765\n",
      "Minibatch accuracy: 49.8%\n",
      "Validation accuracy: 56.4%\n",
      "Minibatch loss at step 7000: 2.260230\n",
      "Minibatch accuracy: 47.1%\n",
      "Validation accuracy: 51.1%\n",
      "Minibatch loss at step 7500: 2.057168\n",
      "Minibatch accuracy: 50.2%\n",
      "Validation accuracy: 57.6%\n",
      "Minibatch loss at step 8000: 1.809337\n",
      "Minibatch accuracy: 54.5%\n",
      "Validation accuracy: 59.9%\n",
      "Minibatch loss at step 8500: 1.812867\n",
      "Minibatch accuracy: 52.3%\n",
      "Validation accuracy: 61.0%\n",
      "Minibatch loss at step 9000: 1.743955\n",
      "Minibatch accuracy: 56.4%\n",
      "Validation accuracy: 57.9%\n",
      "Minibatch loss at step 9500: 1.593753\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 67.0%\n",
      "Minibatch loss at step 10000: 1.407484\n",
      "Minibatch accuracy: 66.6%\n",
      "Validation accuracy: 64.6%\n",
      "Minibatch loss at step 10500: 1.619593\n",
      "Minibatch accuracy: 57.2%\n",
      "Validation accuracy: 55.8%\n",
      "Minibatch loss at step 11000: 1.713054\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 61.3%\n",
      "Minibatch loss at step 11500: 1.453234\n",
      "Minibatch accuracy: 64.8%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 12000: 1.616719\n",
      "Minibatch accuracy: 56.6%\n",
      "Validation accuracy: 66.9%\n",
      "Minibatch loss at step 12500: 1.523769\n",
      "Minibatch accuracy: 62.5%\n",
      "Validation accuracy: 65.5%\n",
      "Minibatch loss at step 13000: 1.443846\n",
      "Minibatch accuracy: 65.4%\n",
      "Validation accuracy: 70.4%\n",
      "Minibatch loss at step 13500: 1.436810\n",
      "Minibatch accuracy: 64.3%\n",
      "Validation accuracy: 64.7%\n",
      "Minibatch loss at step 14000: 1.399480\n",
      "Minibatch accuracy: 67.8%\n",
      "Validation accuracy: 63.4%\n",
      "Minibatch loss at step 14500: 1.330761\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 68.8%\n",
      "Minibatch loss at step 15000: 1.399776\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 72.3%\n",
      "Minibatch loss at step 15500: 1.167058\n",
      "Minibatch accuracy: 75.2%\n",
      "Validation accuracy: 72.0%\n",
      "Minibatch loss at step 16000: 1.478280\n",
      "Minibatch accuracy: 60.2%\n",
      "Validation accuracy: 68.2%\n",
      "Minibatch loss at step 16500: 1.380570\n",
      "Minibatch accuracy: 67.8%\n",
      "Validation accuracy: 70.1%\n",
      "Minibatch loss at step 17000: 1.341013\n",
      "Minibatch accuracy: 66.0%\n",
      "Validation accuracy: 68.9%\n",
      "Minibatch loss at step 17500: 1.147889\n",
      "Minibatch accuracy: 77.0%\n",
      "Validation accuracy: 72.2%\n",
      "Minibatch loss at step 18000: 1.310975\n",
      "Minibatch accuracy: 67.0%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 18500: 1.228305\n",
      "Minibatch accuracy: 72.5%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 19000: 1.172945\n",
      "Minibatch accuracy: 73.0%\n",
      "Validation accuracy: 72.8%\n",
      "Minibatch loss at step 19500: 1.566355\n",
      "Minibatch accuracy: 57.6%\n",
      "Validation accuracy: 70.8%\n",
      "Minibatch loss at step 20000: 1.329124\n",
      "Minibatch accuracy: 66.8%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 20500: 1.070547\n",
      "Minibatch accuracy: 76.6%\n",
      "Validation accuracy: 77.2%\n",
      "Minibatch loss at step 21000: 1.264928\n",
      "Minibatch accuracy: 70.1%\n",
      "Validation accuracy: 70.6%\n",
      "Minibatch loss at step 21500: 1.158365\n",
      "Minibatch accuracy: 73.6%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 22000: 1.123966\n",
      "Minibatch accuracy: 75.6%\n",
      "Validation accuracy: 75.7%\n",
      "Minibatch loss at step 22500: 1.109861\n",
      "Minibatch accuracy: 77.0%\n",
      "Validation accuracy: 78.9%\n",
      "Minibatch loss at step 23000: 1.120570\n",
      "Minibatch accuracy: 74.8%\n",
      "Validation accuracy: 78.3%\n",
      "Minibatch loss at step 23500: 1.128917\n",
      "Minibatch accuracy: 75.0%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 24000: 1.168917\n",
      "Minibatch accuracy: 74.0%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 24500: 1.219395\n",
      "Minibatch accuracy: 72.7%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 25000: 1.221843\n",
      "Minibatch accuracy: 71.7%\n",
      "Validation accuracy: 77.4%\n",
      "Minibatch loss at step 25500: 1.088065\n",
      "Minibatch accuracy: 76.8%\n",
      "Validation accuracy: 77.3%\n",
      "Minibatch loss at step 26000: 1.192593\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 78.8%\n",
      "Minibatch loss at step 26500: 1.064327\n",
      "Minibatch accuracy: 77.7%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 27000: 1.125132\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 27500: 1.000839\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 28000: 1.065619\n",
      "Minibatch accuracy: 78.5%\n",
      "Validation accuracy: 81.5%\n",
      "Minibatch loss at step 28500: 1.102757\n",
      "Minibatch accuracy: 74.6%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 29000: 1.220434\n",
      "Minibatch accuracy: 70.3%\n",
      "Validation accuracy: 76.9%\n",
      "Minibatch loss at step 29500: 1.245648\n",
      "Minibatch accuracy: 69.7%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 30000: 0.978525\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 80.7%\n",
      "Minibatch loss at step 30500: 0.976228\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 82.1%\n",
      "Minibatch loss at step 31000: 1.026069\n",
      "Minibatch accuracy: 78.5%\n",
      "Validation accuracy: 81.6%\n",
      "Minibatch loss at step 31500: 1.077056\n",
      "Minibatch accuracy: 77.9%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 32000: 0.914594\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 82.5%\n",
      "Minibatch loss at step 32500: 1.013910\n",
      "Minibatch accuracy: 82.4%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 33000: 1.088400\n",
      "Minibatch accuracy: 76.2%\n",
      "Validation accuracy: 80.1%\n",
      "Minibatch loss at step 33500: 1.003553\n",
      "Minibatch accuracy: 79.9%\n",
      "Validation accuracy: 81.9%\n",
      "Minibatch loss at step 34000: 0.990693\n",
      "Minibatch accuracy: 80.3%\n",
      "Validation accuracy: 80.3%\n",
      "Minibatch loss at step 34500: 1.125595\n",
      "Minibatch accuracy: 74.8%\n",
      "Validation accuracy: 80.4%\n",
      "Minibatch loss at step 35000: 0.943789\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 83.4%\n",
      "Minibatch loss at step 35500: 0.913775\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 36000: 0.988866\n",
      "Minibatch accuracy: 77.5%\n",
      "Validation accuracy: 82.8%\n",
      "Minibatch loss at step 36500: 0.977773\n",
      "Minibatch accuracy: 80.1%\n",
      "Validation accuracy: 80.6%\n",
      "Minibatch loss at step 37000: 0.948460\n",
      "Minibatch accuracy: 81.2%\n",
      "Validation accuracy: 83.3%\n",
      "Minibatch loss at step 37500: 1.053324\n",
      "Minibatch accuracy: 77.1%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 38000: 0.833762\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 38500: 0.982886\n",
      "Minibatch accuracy: 79.5%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 39000: 1.003376\n",
      "Minibatch accuracy: 78.5%\n",
      "Validation accuracy: 82.4%\n",
      "Minibatch loss at step 39500: 0.882307\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 40000: 0.947930\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 40500: 0.956772\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 41000: 0.898524\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 84.1%\n",
      "Minibatch loss at step 41500: 0.860525\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 42000: 0.964066\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 42500: 1.003515\n",
      "Minibatch accuracy: 79.1%\n",
      "Validation accuracy: 84.6%\n",
      "Minibatch loss at step 43000: 0.921583\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 83.7%\n",
      "Minibatch loss at step 43500: 0.952084\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 84.5%\n",
      "Minibatch loss at step 44000: 0.922542\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 83.5%\n",
      "Minibatch loss at step 44500: 0.894960\n",
      "Minibatch accuracy: 83.0%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 45000: 0.871774\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.0%\n",
      "Minibatch loss at step 45500: 1.000643\n",
      "Minibatch accuracy: 79.3%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 46000: 0.965491\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 82.7%\n",
      "Minibatch loss at step 46500: 0.929517\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 47000: 0.871889\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 47500: 0.885884\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 48000: 0.973257\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 83.6%\n",
      "Minibatch loss at step 48500: 0.883212\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 49000: 0.914891\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 84.8%\n",
      "Minibatch loss at step 49500: 0.949628\n",
      "Minibatch accuracy: 80.7%\n",
      "Validation accuracy: 85.2%\n",
      "Minibatch loss at step 50000: 0.914144\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 84.9%\n",
      "Minibatch loss at step 50500: 0.902706\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 84.4%\n",
      "Minibatch loss at step 51000: 0.933558\n",
      "Minibatch accuracy: 81.1%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 51500: 0.925003\n",
      "Minibatch accuracy: 80.7%\n",
      "Validation accuracy: 84.3%\n",
      "Minibatch loss at step 52000: 0.841255\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 85.0%\n",
      "Minibatch loss at step 52500: 0.913038\n",
      "Minibatch accuracy: 81.6%\n",
      "Validation accuracy: 84.7%\n",
      "Minibatch loss at step 53000: 0.930317\n",
      "Minibatch accuracy: 80.7%\n",
      "Validation accuracy: 84.2%\n",
      "Minibatch loss at step 53500: 0.815427\n",
      "Minibatch accuracy: 86.7%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 54000: 0.894865\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 54500: 0.845047\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 55000: 0.880162\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 55500: 0.916553\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 56000: 0.847165\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 56500: 0.855755\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 85.5%\n",
      "Minibatch loss at step 57000: 0.944321\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 85.9%\n",
      "Minibatch loss at step 57500: 0.880015\n",
      "Minibatch accuracy: 82.6%\n",
      "Validation accuracy: 85.6%\n",
      "Minibatch loss at step 58000: 0.875146\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 58500: 0.914337\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 59000: 0.877800\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 59500: 0.892051\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 60000: 0.866746\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 60500: 0.834271\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 61000: 0.868537\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 61500: 0.818716\n",
      "Minibatch accuracy: 85.7%\n",
      "Validation accuracy: 85.1%\n",
      "Minibatch loss at step 62000: 0.921788\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 62500: 0.821081\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 63000: 0.813762\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 63500: 0.850072\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 85.3%\n",
      "Minibatch loss at step 64000: 0.836684\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 85.8%\n",
      "Minibatch loss at step 64500: 0.837956\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 65000: 0.840969\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 65500: 0.866736\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 66000: 0.848368\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 66500: 0.896484\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 67000: 0.806445\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 86.0%\n",
      "Minibatch loss at step 67500: 0.908994\n",
      "Minibatch accuracy: 80.5%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 68000: 0.837953\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.1%\n",
      "Minibatch loss at step 68500: 0.794460\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 69000: 0.832361\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 69500: 0.829961\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 70000: 0.774012\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 70500: 0.866363\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 71000: 0.774982\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 71500: 0.787717\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.2%\n",
      "Minibatch loss at step 72000: 0.930636\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 72500: 0.841698\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 73000: 0.812124\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 73500: 0.858212\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 74000: 0.811003\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 74500: 0.809775\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 75000: 0.856090\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 86.3%\n",
      "Minibatch loss at step 75500: 0.833650\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 76000: 0.793835\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 76500: 0.862734\n",
      "Minibatch accuracy: 83.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 77000: 0.857216\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 77500: 0.824321\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 78000: 0.790596\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 78500: 0.860638\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 79000: 0.828793\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 79500: 0.810627\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 80000: 0.832156\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 80500: 0.806109\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 86.4%\n",
      "Minibatch loss at step 81000: 0.790542\n",
      "Minibatch accuracy: 86.9%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 81500: 0.847139\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 82000: 0.870840\n",
      "Minibatch accuracy: 83.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 82500: 0.797924\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 83000: 0.919707\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 83500: 0.817121\n",
      "Minibatch accuracy: 85.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 84000: 0.872094\n",
      "Minibatch accuracy: 82.8%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 84500: 0.831710\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 86.5%\n",
      "Minibatch loss at step 85000: 0.772704\n",
      "Minibatch accuracy: 87.7%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 85500: 0.792439\n",
      "Minibatch accuracy: 85.5%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 86000: 0.871420\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 86500: 0.765841\n",
      "Minibatch accuracy: 87.9%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 87000: 0.798843\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 87500: 0.818479\n",
      "Minibatch accuracy: 83.6%\n",
      "Validation accuracy: 86.6%\n",
      "Minibatch loss at step 88000: 0.731467\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 86.7%\n",
      "Minibatch loss at step 88500: 0.925586\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 89000: 0.797564\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 86.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at step 89500: 0.827114\n",
      "Minibatch accuracy: 84.8%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 90000: 0.856255\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 90500: 0.803959\n",
      "Minibatch accuracy: 86.1%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 91000: 0.809474\n",
      "Minibatch accuracy: 86.5%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 91500: 0.886683\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 92000: 0.822374\n",
      "Minibatch accuracy: 85.7%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 92500: 0.807177\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 93000: 0.851129\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 93500: 0.828013\n",
      "Minibatch accuracy: 84.0%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 94000: 0.831221\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 94500: 0.742679\n",
      "Minibatch accuracy: 88.3%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 95000: 0.867718\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 95500: 0.783981\n",
      "Minibatch accuracy: 85.4%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 96000: 0.812818\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 96500: 0.839807\n",
      "Minibatch accuracy: 84.2%\n",
      "Validation accuracy: 87.1%\n",
      "Minibatch loss at step 97000: 0.790669\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 97500: 0.813308\n",
      "Minibatch accuracy: 85.2%\n",
      "Validation accuracy: 87.0%\n",
      "Minibatch loss at step 98000: 0.845732\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 98500: 0.822228\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 99000: 0.799044\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 86.9%\n",
      "Minibatch loss at step 99500: 0.831468\n",
      "Minibatch accuracy: 83.4%\n",
      "Validation accuracy: 86.8%\n",
      "Minibatch loss at step 100000: 0.872402\n",
      "Minibatch accuracy: 83.0%\n",
      "Validation accuracy: 87.0%\n",
      "Test accuracy: 84.6%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 0.8}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 8 32\n",
      "8 8 32\n",
      "8 8 32\n"
     ]
    }
   ],
   "source": [
    "# CNN\n",
    "batch_size = 64\n",
    "patch_size = 3\n",
    "depth_1 = 16\n",
    "depth_2 = 16\n",
    "depth_3 = 16\n",
    "depth_4 = 32\n",
    "num_hidden = 64\n",
    "\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data.\n",
    "    tf_train_dataset = tf.placeholder(\n",
    "    tf.float32, shape=(batch_size, image_size, image_size, num_channels))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    layer1_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, num_channels, depth_1], stddev=0.1))\n",
    "    layer1_biases = tf.Variable(tf.zeros([depth_1]))\n",
    "\n",
    "    layer2_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth_1, depth_2], stddev=0.1))\n",
    "    layer2_biases = tf.Variable(tf.constant(1.0, shape=[depth_2]))\n",
    "\n",
    "    layer3_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth_2, depth_3], stddev=0.1))\n",
    "    layer3_biases = tf.Variable(tf.constant(1.0, shape=[depth_3])) \n",
    "    \n",
    "    layer4_weights = tf.Variable(tf.truncated_normal(\n",
    "      [patch_size, patch_size, depth_3, depth_4], stddev=0.1))\n",
    "    layer4_biases = tf.Variable(tf.constant(1.0, shape=[depth_4]))  \n",
    "    \n",
    "    layer5_weights = tf.Variable(tf.truncated_normal(\n",
    "      [image_size // 4 * image_size // 4 * depth_4, num_hidden], stddev=0.1))\n",
    "    layer5_biases = tf.Variable(tf.constant(1.0, shape=[num_hidden]))\n",
    "\n",
    "    layer6_weights = tf.Variable(tf.truncated_normal(\n",
    "      [num_hidden, num_labels], stddev=0.1))\n",
    "    layer6_biases = tf.Variable(tf.constant(1.0, shape=[num_labels]))\n",
    "  \n",
    "  # Model.\n",
    "    def model(data):\n",
    "        conv = tf.nn.conv2d(data, layer1_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(tf.add(conv, layer1_biases))\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, layer2_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        max_pol = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        hidden = tf.nn.relu(tf.add(max_pol, layer2_biases))\n",
    "        \n",
    "\n",
    "        conv = tf.nn.conv2d(hidden, layer3_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        hidden = tf.nn.relu(tf.add(conv, layer3_biases))\n",
    "        \n",
    "        conv = tf.nn.conv2d(hidden, layer4_weights, [1, 1, 1, 1], padding='SAME')\n",
    "        max_pol = tf.nn.max_pool(conv, [1, 2, 2, 1], [1, 2, 2, 1], padding='VALID')\n",
    "        hidden = tf.nn.relu(tf.add(max_pol, layer4_biases))\n",
    "        \n",
    "        shape = hidden.get_shape().as_list()\n",
    "        \n",
    "        print(shape[1], shape[2], shape[3])\n",
    "        \n",
    "        reshape = tf.reshape(hidden, [shape[0], shape[1] * shape[2] * shape[3]])\n",
    "        hidden = tf.nn.relu(tf.add(tf.matmul(reshape, layer5_weights), layer5_biases))\n",
    "        return tf.add(tf.matmul(hidden, layer6_weights), layer6_biases)\n",
    "\n",
    "    # Training computation.\n",
    "    logits = model(tf_train_dataset)\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Optimizer.\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.05).minimize(loss)\n",
    "    #   optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "#     global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "#     start_learning_rate = 0.1\n",
    "#     learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 100, 0.98)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(model(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 9.941973\n",
      "Minibatch accuracy: 12.5%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 50: 2.254923\n",
      "Minibatch accuracy: 17.2%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 100: 2.220489\n",
      "Minibatch accuracy: 23.4%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 150: 2.254634\n",
      "Minibatch accuracy: 18.8%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 200: 2.189073\n",
      "Minibatch accuracy: 21.9%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 250: 2.195249\n",
      "Minibatch accuracy: 23.4%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 300: 2.230347\n",
      "Minibatch accuracy: 23.4%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 350: 2.242455\n",
      "Minibatch accuracy: 25.0%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 400: 2.237279\n",
      "Minibatch accuracy: 15.6%\n",
      "Validation accuracy: 19.2%\n",
      "Minibatch loss at step 450: 2.131022\n",
      "Minibatch accuracy: 26.6%\n",
      "Validation accuracy: 19.2%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-4de409bba027>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     _, l, predictions = session.run(\n\u001b[1;32m---> 12\u001b[1;33m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     13\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m50\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Minibatch loss at step %d: %f'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print('Initialized')\n",
    "  for step in range(num_steps):\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :, :, :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 50 == 0):\n",
    "      print('Minibatch loss at step %d: %f' % (step, l))\n",
    "      print('Minibatch accuracy: %.1f%%' % accuracy(predictions, batch_labels))\n",
    "      print('Validation accuracy: %.1f%%' % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print('Test accuracy: %.1f%%' % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# layers\n",
    "\n",
    "batch_size = 1024\n",
    "hidden_units_1 = 1024\n",
    "hidden_units_2 = 128\n",
    "lambd = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "    # Input data. For the training data, we use a placeholder that will be fed\n",
    "    # at run time with a training minibatch.\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "\n",
    "    # Variables.\n",
    "    weights1 = tf.Variable(tf.truncated_normal([image_size * image_size, hidden_units_1]))\n",
    "    biases1 = tf.Variable(tf.zeros([hidden_units_1]))\n",
    "    \n",
    "    weights2 = tf.Variable(tf.truncated_normal([hidden_units_1, hidden_units_2]))\n",
    "    biases2 = tf.Variable(tf.zeros([hidden_units_2]))\n",
    "    \n",
    "    weights3 = tf.Variable(\n",
    "    tf.truncated_normal([hidden_units_2, num_labels]))\n",
    "    biases3 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    \n",
    "    keep_prob = tf.placeholder(tf.float32)\n",
    "    \n",
    "      \n",
    "    parametrs = {'weights1': weights1,\n",
    "                 'weights2': weights2, \n",
    "                 'weights3': weights3,\n",
    "                 'biases1': biases1,\n",
    "                 'biases2': biases2,\n",
    "                 'biases3': biases3}\n",
    "\n",
    "    # Training computation.\n",
    "\n",
    "    def forward_propagation(tf_dataset, keep_prob, parametrs):\n",
    "      fist_level = tf.nn.relu(tf.add(tf.matmul(tf_dataset, parametrs['weights1']), parametrs['biases1']))\n",
    "      drop_out = tf.nn.dropout(fist_level, keep_prob)\n",
    "      second_level = tf.nn.relu(tf.add(tf.matmul(drop_out, parametrs['weights2']), parametrs['biases2']))\n",
    "      drop_out = tf.nn.dropout(second_level, keep_prob)\n",
    "      out = tf.add(tf.matmul(drop_out, parametrs['weights3']), parametrs['biases3'])\n",
    "      return out\n",
    "\n",
    "    logits = forward_propagation(tf_train_dataset, keep_prob, parametrs)\n",
    "\n",
    "    loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf_train_labels, logits=logits))\n",
    "\n",
    "    # Loss function using L2 Regularization\n",
    "    regularizer = tf.nn.l2_loss(weights1) + tf.nn.l2_loss(weights2) + tf.nn.l2_loss(weights3)\n",
    "    \n",
    "    loss = tf.reduce_mean(loss + lambd * regularizer)\n",
    "\n",
    "    # Optimizer.\n",
    "    # Decaying learning rate\n",
    "#     global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "#     start_learning_rate = 0.6\n",
    "#     learning_rate = tf.train.exponential_decay(start_learning_rate, global_step, 50000, 0.85)\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "\n",
    "#     optimizer = tf.train.GradientDescentOptimizer(0.001).minimize(loss)\n",
    "\n",
    "    optimizer = tf.train.AdamOptimizer(learning_rate=0.001).minimize(loss)\n",
    "\n",
    "    # Predictions for the training, validation, and test data.\n",
    "    train_prediction = tf.nn.softmax(logits)\n",
    "    valid_prediction = tf.nn.softmax(forward_propagation(tf_valid_dataset, 1, parametrs))\n",
    "    test_prediction = tf.nn.softmax(forward_propagation(tf_test_dataset, 1, parametrs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 3920.625488\n",
      "Minibatch accuracy: 7.2%\n",
      "Validation accuracy: 9.5%\n",
      "Minibatch loss at step 500: 481.748169\n",
      "Minibatch accuracy: 42.1%\n",
      "Validation accuracy: 39.9%\n",
      "Minibatch loss at step 1000: 427.888763\n",
      "Minibatch accuracy: 52.5%\n",
      "Validation accuracy: 50.7%\n",
      "Minibatch loss at step 1500: 403.416687\n",
      "Minibatch accuracy: 55.9%\n",
      "Validation accuracy: 53.2%\n",
      "Minibatch loss at step 2000: 381.537720\n",
      "Minibatch accuracy: 60.7%\n",
      "Validation accuracy: 57.9%\n",
      "Minibatch loss at step 2500: 370.671326\n",
      "Minibatch accuracy: 61.8%\n",
      "Validation accuracy: 59.5%\n",
      "Minibatch loss at step 3000: 365.077026\n",
      "Minibatch accuracy: 57.8%\n",
      "Validation accuracy: 55.9%\n",
      "Minibatch loss at step 3500: 345.178253\n",
      "Minibatch accuracy: 68.4%\n",
      "Validation accuracy: 64.2%\n",
      "Minibatch loss at step 4000: 428.325073\n",
      "Minibatch accuracy: 37.1%\n",
      "Validation accuracy: 50.5%\n",
      "Minibatch loss at step 4500: 325.672089\n",
      "Minibatch accuracy: 71.9%\n",
      "Validation accuracy: 67.2%\n",
      "Minibatch loss at step 5000: 318.657104\n",
      "Minibatch accuracy: 72.1%\n",
      "Validation accuracy: 68.5%\n",
      "Minibatch loss at step 5500: 308.770874\n",
      "Minibatch accuracy: 73.1%\n",
      "Validation accuracy: 70.0%\n",
      "Minibatch loss at step 6000: 382.984070\n",
      "Minibatch accuracy: 43.0%\n",
      "Validation accuracy: 54.3%\n",
      "Minibatch loss at step 6500: 291.553680\n",
      "Minibatch accuracy: 76.4%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 7000: 286.388611\n",
      "Minibatch accuracy: 74.1%\n",
      "Validation accuracy: 68.4%\n",
      "Minibatch loss at step 7500: 278.745544\n",
      "Minibatch accuracy: 76.0%\n",
      "Validation accuracy: 72.1%\n",
      "Minibatch loss at step 8000: 272.575958\n",
      "Minibatch accuracy: 76.7%\n",
      "Validation accuracy: 72.5%\n",
      "Minibatch loss at step 8500: 265.823120\n",
      "Minibatch accuracy: 79.2%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 9000: 261.431580\n",
      "Minibatch accuracy: 79.6%\n",
      "Validation accuracy: 73.2%\n",
      "Minibatch loss at step 9500: 255.922684\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 73.8%\n",
      "Minibatch loss at step 10000: 257.327881\n",
      "Minibatch accuracy: 72.2%\n",
      "Validation accuracy: 71.9%\n",
      "Minibatch loss at step 10500: 249.019196\n",
      "Minibatch accuracy: 79.9%\n",
      "Validation accuracy: 72.6%\n",
      "Minibatch loss at step 11000: 248.096756\n",
      "Minibatch accuracy: 75.2%\n",
      "Validation accuracy: 71.2%\n",
      "Minibatch loss at step 11500: 251.046936\n",
      "Minibatch accuracy: 72.1%\n",
      "Validation accuracy: 72.4%\n",
      "Minibatch loss at step 12000: 240.025879\n",
      "Minibatch accuracy: 78.0%\n",
      "Validation accuracy: 75.8%\n",
      "Minibatch loss at step 12500: 236.798004\n",
      "Minibatch accuracy: 79.2%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 13000: 234.661743\n",
      "Minibatch accuracy: 77.2%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 13500: 232.213898\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 14000: 230.356323\n",
      "Minibatch accuracy: 77.8%\n",
      "Validation accuracy: 76.8%\n",
      "Minibatch loss at step 14500: 226.920120\n",
      "Minibatch accuracy: 79.3%\n",
      "Validation accuracy: 73.3%\n",
      "Minibatch loss at step 15000: 227.957092\n",
      "Minibatch accuracy: 74.9%\n",
      "Validation accuracy: 65.0%\n",
      "Minibatch loss at step 15500: 222.508301\n",
      "Minibatch accuracy: 78.0%\n",
      "Validation accuracy: 76.7%\n",
      "Minibatch loss at step 16000: 219.182159\n",
      "Minibatch accuracy: 80.2%\n",
      "Validation accuracy: 76.6%\n",
      "Minibatch loss at step 16500: 216.217026\n",
      "Minibatch accuracy: 77.1%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 17000: 213.469101\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 75.3%\n",
      "Minibatch loss at step 17500: 278.095825\n",
      "Minibatch accuracy: 47.2%\n",
      "Validation accuracy: 43.2%\n",
      "Minibatch loss at step 18000: 208.322922\n",
      "Minibatch accuracy: 82.3%\n",
      "Validation accuracy: 77.8%\n",
      "Minibatch loss at step 18500: 206.093338\n",
      "Minibatch accuracy: 82.2%\n",
      "Validation accuracy: 77.0%\n",
      "Minibatch loss at step 19000: 203.427994\n",
      "Minibatch accuracy: 84.9%\n",
      "Validation accuracy: 78.0%\n",
      "Minibatch loss at step 19500: 200.409668\n",
      "Minibatch accuracy: 86.3%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 20000: 199.596558\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 20500: 199.698059\n",
      "Minibatch accuracy: 80.8%\n",
      "Validation accuracy: 76.3%\n",
      "Minibatch loss at step 21000: 196.117065\n",
      "Minibatch accuracy: 81.9%\n",
      "Validation accuracy: 78.4%\n",
      "Minibatch loss at step 21500: 193.946335\n",
      "Minibatch accuracy: 82.3%\n",
      "Validation accuracy: 76.5%\n",
      "Minibatch loss at step 22000: 192.446945\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 78.1%\n",
      "Minibatch loss at step 22500: 189.596405\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 77.6%\n",
      "Minibatch loss at step 23000: 190.355438\n",
      "Minibatch accuracy: 76.5%\n",
      "Validation accuracy: 73.7%\n",
      "Minibatch loss at step 23500: 185.907089\n",
      "Minibatch accuracy: 81.7%\n",
      "Validation accuracy: 76.2%\n",
      "Minibatch loss at step 24000: 184.250427\n",
      "Minibatch accuracy: 81.8%\n",
      "Validation accuracy: 73.4%\n",
      "Minibatch loss at step 24500: 181.670059\n",
      "Minibatch accuracy: 84.6%\n",
      "Validation accuracy: 78.6%\n",
      "Minibatch loss at step 25000: 179.808258\n",
      "Minibatch accuracy: 85.1%\n",
      "Validation accuracy: 78.7%\n",
      "Minibatch loss at step 25500: 180.211914\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 74.1%\n",
      "Minibatch loss at step 26000: 176.469101\n",
      "Minibatch accuracy: 83.9%\n",
      "Validation accuracy: 78.2%\n",
      "Minibatch loss at step 26500: 176.815475\n",
      "Minibatch accuracy: 79.4%\n",
      "Validation accuracy: 76.7%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-04cf8f90f218>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mtf_train_dataset\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf_train_labels\u001b[0m \u001b[1;33m:\u001b[0m \u001b[0mbatch_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_prob\u001b[0m \u001b[1;33m:\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     _, l, predictions = session.run(\n\u001b[1;32m---> 18\u001b[1;33m       [optimizer, loss, train_prediction], feed_dict=feed_dict)\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m500\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m       \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Minibatch loss at step %d: %f\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    875\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 877\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    878\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1098\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[1;32m-> 1100\u001b[1;33m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[0;32m   1101\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1270\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[1;32m-> 1272\u001b[1;33m                            run_metadata)\n\u001b[0m\u001b[0;32m   1273\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1274\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m   1276\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1277\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1278\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1279\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[1;32m-> 1263\u001b[1;33m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[0;32m   1264\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\учёба\\магистратура\\____аспирантура\\___keras\\venv\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[1;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[0;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[0;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1350\u001b[1;33m         run_metadata)\n\u001b[0m\u001b[0;32m   1351\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1352\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_steps = 10001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.global_variables_initializer().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, keep_prob : 1}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
